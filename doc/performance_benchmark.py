"""
æ€§èƒ½åŸºå‡†æµ‹è¯• / Performance Benchmark
====================================

è¯¦ç»†çš„æ€§èƒ½æµ‹è¯•ï¼ŒåŒ…æ‹¬ï¼š
- å„ä¸ªæ“ä½œçš„æ‰§è¡Œæ—¶é—´
- ä¸åŒå‘é‡å¤§å°çš„æ€§èƒ½æ‰©å±•
- å†…å­˜ä½¿ç”¨æƒ…å†µ
- æ€§èƒ½å¯¹æ¯”åˆ†æ

è¿è¡Œæ–¹å¼ï¼š
    python performance_benchmark.py
"""

import time
import json
import sys
import os
from typing import Dict, List, Tuple
import tracemalloc

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥ vc_smallness
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from vc_smallness import setup, keygen_crs
from vc_smallness.commit import commit_G, commit_Ghat, commit_Cy, commit_V
from vc_smallness.proofs import (
    prove_point_open, prove_agg_open, prove_eq, prove_y, prove_x, aggregate_pi
)
from vc_smallness.verify import verify_1, verify_5, verify_7, verify_9, verify_16
from vc_smallness.fs_oracles import H_t, H_agg, H_s
from charm.toolbox.pairinggroup import ZR
from charm.core.engine.util import objectToBytes


class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•ç±»"""
    
    def __init__(self, curve='MNT224'):
        """åˆå§‹åŒ–åŸºå‡†æµ‹è¯•"""
        print(f"ğŸ”§ åˆå§‹åŒ–æ€§èƒ½æµ‹è¯• (æ›²çº¿: {curve})...")
        self.params = setup(curve)
        self.group = self.params['group']
        self.results = {}
        self.memory_results = {}
        
    def measure_time(self, func, *args, num_runs=10, **kwargs) -> Tuple[float, float, any]:
        """
        æµ‹é‡å‡½æ•°æ‰§è¡Œæ—¶é—´çš„å¹³å‡å€¼å’Œæ ‡å‡†å·®ï¼ˆç§’ï¼‰

        Args:
            func: è¦æµ‹è¯•çš„å‡½æ•°
            num_runs: é‡å¤æµ‹è¯•æ¬¡æ•°ï¼ˆé»˜è®¤10æ¬¡ï¼‰
            *args, **kwargs: ä¼ é€’ç»™å‡½æ•°çš„å‚æ•°

        Returns:
            (å¹³å‡æ—¶é—´, æ ‡å‡†å·®, æœ€åä¸€æ¬¡æ‰§è¡Œç»“æœ)
        """
        times = []
        result = None

        for _ in range(num_runs):
            start = time.perf_counter()
            result = func(*args, **kwargs)
            end = time.perf_counter()
            times.append(end - start)

        avg_time = sum(times) / len(times)
        std_dev = (sum((t - avg_time)**2 for t in times) / len(times)) ** 0.5

        return avg_time, std_dev, result
    
    def measure_memory(self, func, *args, **kwargs) -> Tuple[float, any]:
        """æµ‹é‡å‡½æ•°å†…å­˜ä½¿ç”¨ï¼ˆMBï¼‰"""
        tracemalloc.start()
        result = func(*args, **kwargs)
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        return peak / 1024 / 1024, result  # è½¬æ¢ä¸º MB
    
    def benchmark_crs_generation(self, vector_sizes: List[int], num_runs=10):
        """åŸºå‡†æµ‹è¯• CRS ç”Ÿæˆ"""
        print("\nğŸ“Š CRS ç”Ÿæˆæ€§èƒ½æµ‹è¯• (æ¯ä¸ªæµ‹è¯•é‡å¤ {} æ¬¡)".format(num_runs))
        print("=" * 60)

        results = {}
        std_devs = {}
        for n in vector_sizes:
            print(f"  æµ‹è¯• n={n}...", end=" ", flush=True)
            avg_time, std_dev, crs = self.measure_time(keygen_crs, n, self.group, num_runs=num_runs)
            results[n] = avg_time
            std_devs[n] = std_dev
            print(f"âœ“ {avg_time*1000:.2f} Â± {std_dev*1000:.2f} ms")

        self.results['crs_generation'] = results
        self.results['crs_generation_std'] = std_devs
        return results
    
    def benchmark_commitments(self, vector_sizes: List[int], num_runs=10):
        """åŸºå‡†æµ‹è¯•æ‰¿è¯ºç”Ÿæˆ"""
        print("\nğŸ“Š æ‰¿è¯ºç”Ÿæˆæ€§èƒ½æµ‹è¯• (æ¯ä¸ªæµ‹è¯•é‡å¤ {} æ¬¡)".format(num_runs))
        print("=" * 60)

        results = {'commit_G': {}, 'commit_Ghat': {}, 'commit_Cy': {}, 'commit_V': {}}
        std_devs = {'commit_G': {}, 'commit_Ghat': {}, 'commit_Cy': {}, 'commit_V': {}}

        for n in vector_sizes:
            print(f"  æµ‹è¯• n={n}...", end=" ", flush=True)
            crs = keygen_crs(n, self.group)

            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            m = [self.group.random(ZR) for _ in range(n)]
            x = [self.group.random(ZR) for _ in range(n)]
            y = [self.group.init(ZR, i % 2) for i in range(n)]  # äºŒè¿›åˆ¶å‘é‡
            gamma = self.group.random(ZR)
            gamma_y = self.group.random(ZR)

            # æµ‹è¯•å„ä¸ªæ‰¿è¯º
            t1, s1, _ = self.measure_time(commit_G, m, gamma, crs, num_runs=num_runs)
            t2, s2, _ = self.measure_time(commit_Ghat, x, gamma, crs, num_runs=num_runs)
            t3, s3, _ = self.measure_time(commit_Cy, y, x, gamma_y, crs, num_runs=num_runs)
            t4, s4, _ = self.measure_time(commit_V, self.group.init(ZR, 42), gamma, crs, num_runs=num_runs)

            results['commit_G'][n] = t1
            results['commit_Ghat'][n] = t2
            results['commit_Cy'][n] = t3
            results['commit_V'][n] = t4

            std_devs['commit_G'][n] = s1
            std_devs['commit_Ghat'][n] = s2
            std_devs['commit_Cy'][n] = s3
            std_devs['commit_V'][n] = s4

            print(f"âœ“ G:{t1*1000:.2f}Â±{s1*1000:.2f}ms Äœ:{t2*1000:.2f}Â±{s2*1000:.2f}ms Cy:{t3*1000:.2f}Â±{s3*1000:.2f}ms V:{t4*1000:.2f}Â±{s4*1000:.2f}ms")

        self.results['commitments'] = results
        self.results['commitments_std'] = std_devs
        return results
    
    def benchmark_proofs(self, vector_sizes: List[int], num_runs=10):
        """åŸºå‡†æµ‹è¯•è¯æ˜ç”Ÿæˆ"""
        print("\nğŸ“Š è¯æ˜ç”Ÿæˆæ€§èƒ½æµ‹è¯• (æ¯ä¸ªæµ‹è¯•é‡å¤ {} æ¬¡)".format(num_runs))
        print("=" * 60)

        results = {
            'point_open': {},
            'agg_open': {},
            'equality': {},
            'orthogonality': {},
            'range': {}
        }
        std_devs = {
            'point_open': {},
            'agg_open': {},
            'equality': {},
            'orthogonality': {},
            'range': {}
        }

        for n in vector_sizes:
            print(f"  æµ‹è¯• n={n}...", end=" ", flush=True)
            crs = keygen_crs(n, self.group)

            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            m = [self.group.random(ZR) for _ in range(n)]
            x = [self.group.random(ZR) for _ in range(n)]
            y = [self.group.init(ZR, i % 2) for i in range(n)]
            gamma = self.group.random(ZR)
            gamma_y = self.group.random(ZR)

            # ç”Ÿæˆæ‰¿è¯º
            C = commit_G(m, gamma, crs)
            C_hat = commit_Ghat(x, gamma, crs)
            C_y = commit_Cy(y, x, gamma_y, crs)

            # ç‚¹å¼€æ”¾è¯æ˜
            t1, s1, pi_i = self.measure_time(prove_point_open, C, m, gamma, 1, crs, num_runs=num_runs)

            # èšåˆå¼€æ”¾è¯æ˜
            t = [self.group.random(ZR) for _ in range(n)]
            t2, s2, pi_agg = self.measure_time(prove_agg_open, C, m, gamma, list(range(1, n+1)), t, crs, num_runs=num_runs)

            # ç­‰å¼è¯æ˜
            t3, s3, pi_eq = self.measure_time(prove_eq, t, y, x, gamma, gamma_y, crs, num_runs=num_runs)

            # æ­£äº¤æ€§è¯æ˜
            t4, s4, pi_y = self.measure_time(prove_y, x, y, gamma, gamma_y, crs, num_runs=num_runs)

            # èŒƒå›´è¯æ˜
            bit_proofs = [prove_point_open(C_hat, x, gamma, i, crs) for i in range(1, min(n+1, 9))]
            t5, s5, pi_x = self.measure_time(prove_x, bit_proofs, gamma, crs, num_runs=num_runs)

            results['point_open'][n] = t1
            results['agg_open'][n] = t2
            results['equality'][n] = t3
            results['orthogonality'][n] = t4
            results['range'][n] = t5

            std_devs['point_open'][n] = s1
            std_devs['agg_open'][n] = s2
            std_devs['equality'][n] = s3
            std_devs['orthogonality'][n] = s4
            std_devs['range'][n] = s5

            print(f"âœ“ PO:{t1*1000:.2f}Â±{s1*1000:.2f}ms AO:{t2*1000:.2f}Â±{s2*1000:.2f}ms EQ:{t3*1000:.2f}Â±{s3*1000:.2f}ms ORT:{t4*1000:.2f}Â±{s4*1000:.2f}ms RNG:{t5*1000:.2f}Â±{s5*1000:.2f}ms")

        self.results['proofs'] = results
        self.results['proofs_std'] = std_devs
        return results
    
    def benchmark_verification(self, vector_sizes: List[int], num_runs=10):
        """
        åŸºå‡†æµ‹è¯•éªŒè¯

        éªŒè¯æ–¹ç¨‹è¯´æ˜:
        - verify_1 (æ–¹ç¨‹1): éªŒè¯ç‚¹å¼€æ”¾è¯æ˜ - éªŒè¯æ‰¿è¯ºCåœ¨ç‰¹å®šä½ç½®çš„å¼€æ”¾æ˜¯å¦æ­£ç¡®
        - verify_5 (æ–¹ç¨‹5): éªŒè¯ç­‰å¼è¯æ˜ - éªŒè¯ Äˆ å’Œ C_y ä¹‹é—´çš„ç­‰å¼å…³ç³»
        - verify_7 (æ–¹ç¨‹7): éªŒè¯æ­£äº¤æ€§è¯æ˜ - éªŒè¯å‘é‡ x å’Œ y çš„æ­£äº¤æ€§ (å†…ç§¯ä¸º0)
        - verify_9 (æ–¹ç¨‹9): éªŒè¯èŒƒå›´è¯æ˜ - éªŒè¯ Äˆ å’Œ VÌ‚ è¡¨ç¤ºåŒä¸€ä¸ªå€¼çš„ä¸åŒè¡¨ç¤º
        - verify_16 (æ–¹ç¨‹16): éªŒè¯èšåˆè¯æ˜ - åŒæ—¶éªŒè¯ç­‰å¼å’Œæ­£äº¤æ€§çš„èšåˆè¯æ˜
        """
        print("\nğŸ“Š éªŒè¯æ€§èƒ½æµ‹è¯• (æ¯ä¸ªæµ‹è¯•é‡å¤ {} æ¬¡)".format(num_runs))
        print("=" * 60)

        results = {
            'verify_1_point_opening': {},
            'verify_5_equality': {},
            'verify_7_orthogonality': {},
            'verify_9_range': {},
            'verify_16_aggregated': {}
        }
        std_devs = {
            'verify_1_point_opening': {},
            'verify_5_equality': {},
            'verify_7_orthogonality': {},
            'verify_9_range': {},
            'verify_16_aggregated': {}
        }

        for n in vector_sizes:
            print(f"  æµ‹è¯• n={n}...", end=" ", flush=True)
            crs = keygen_crs(n, self.group)

            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            m = [self.group.random(ZR) for _ in range(n)]
            x = [self.group.random(ZR) for _ in range(n)]
            y = [self.group.init(ZR, i % 2) for i in range(n)]
            gamma = self.group.random(ZR)
            gamma_y = self.group.random(ZR)

            # ç”Ÿæˆæ‰¿è¯ºå’Œè¯æ˜
            C = commit_G(m, gamma, crs)
            C_hat = commit_Ghat(x, gamma, crs)
            C_y = commit_Cy(y, x, gamma_y, crs)

            t = [self.group.random(ZR) for _ in range(n)]
            pis = [prove_point_open(C, m, gamma, i, crs) for i in range(1, n+1)]
            pi_eq = prove_eq(t, y, x, gamma, gamma_y, crs)
            pi_y = prove_y(x, y, gamma, gamma_y, crs)

            # éªŒè¯
            t1, s1, _ = self.measure_time(verify_1, C, pis, t, m, crs, num_runs=num_runs)
            t2, s2, _ = self.measure_time(verify_5, C_hat, C_y, t, y, pi_eq, crs, num_runs=num_runs)
            t3, s3, _ = self.measure_time(verify_7, C_hat, C_y, pi_y, y, crs, num_runs=num_runs)

            # verify_9 å’Œ verify_16 éœ€è¦ç‰¹æ®Šå¤„ç†
            ell = min(n, 8)
            # åˆ›å»ºå®Œæ•´é•¿åº¦çš„å‘é‡ï¼Œä½†åªæœ‰å‰ ell ä¸ªæ˜¯äºŒè¿›åˆ¶ä½
            x_bits_full = [self.group.init(ZR, i % 2) if i < ell else self.group.init(ZR, 0) for i in range(n)]
            x_scalar = self.group.init(ZR, sum(int(x_bits_full[i]) * (2**i) for i in range(ell)))
            V_hat = commit_V(x_scalar, gamma, crs)
            bit_proofs = [prove_point_open(C_hat, x_bits_full, gamma, i, crs) for i in range(1, ell+1)]
            pi_x = prove_x(bit_proofs, gamma, crs)

            t4, s4, _ = self.measure_time(verify_9, C_hat, V_hat, pi_x, ell, crs, num_runs=num_runs)

            # verify_16
            delta_eq = self.group.random(ZR)
            delta_y = self.group.random(ZR)
            pi = aggregate_pi(pi_eq, pi_y, delta_eq, delta_y, crs)
            t5, s5, _ = self.measure_time(verify_16, C_hat, C_y, pi, delta_eq, delta_y, t, y, crs, num_runs=num_runs)

            results['verify_1_point_opening'][n] = t1
            results['verify_5_equality'][n] = t2
            results['verify_7_orthogonality'][n] = t3
            results['verify_9_range'][n] = t4
            results['verify_16_aggregated'][n] = t5

            std_devs['verify_1_point_opening'][n] = s1
            std_devs['verify_5_equality'][n] = s2
            std_devs['verify_7_orthogonality'][n] = s3
            std_devs['verify_9_range'][n] = s4
            std_devs['verify_16_aggregated'][n] = s5

            print(f"âœ“ V1:{t1*1000:.2f}Â±{s1*1000:.2f}ms V5:{t2*1000:.2f}Â±{s2*1000:.2f}ms V7:{t3*1000:.2f}Â±{s3*1000:.2f}ms V9:{t4*1000:.2f}Â±{s4*1000:.2f}ms V16:{t5*1000:.2f}Â±{s5*1000:.2f}ms")

        self.results['verification'] = results
        self.results['verification_std'] = std_devs
        return results
    
    def benchmark_memory(self, vector_sizes: List[int]):
        """åŸºå‡†æµ‹è¯•å†…å­˜ä½¿ç”¨"""
        print("\nğŸ“Š å†…å­˜ä½¿ç”¨æ€§èƒ½æµ‹è¯•")
        print("=" * 60)

        results = {}
        for n in vector_sizes:
            print(f"  æµ‹è¯• n={n}...", end=" ", flush=True)
            mem, crs = self.measure_memory(keygen_crs, n, self.group)
            results[n] = mem
            print(f"âœ“ {mem:.2f} MB")

        self.memory_results['crs'] = results
        return results

    def benchmark_bandwidth(self, vector_sizes: List[int]):
        """
        åŸºå‡†æµ‹è¯•é€šä¿¡å¸¦å®½/å¼€é”€

        é€šè¿‡åºåˆ—åŒ–å¯¹è±¡å¹¶æµ‹é‡å­—èŠ‚å¤§å°æ¥è¯„ä¼°é€šä¿¡å¼€é”€ã€‚
        è¿™å±•ç¤ºäº† VDS ç³»ç»Ÿçš„æ ¸å¿ƒä¼˜åŠ¿ï¼šè¯æ˜å¤§å°ä¸º O(1)ï¼Œä¸æ•°æ®é‡ N æ— å…³ã€‚
        """
        print("\nğŸ“Š é€šä¿¡å¸¦å®½/å¼€é”€æ€§èƒ½æµ‹è¯•")
        print("=" * 60)

        results = {
            'header_size': {},
            'proof_size': {},
            'raw_data_size': {}
        }

        for n in vector_sizes:
            print(f"  æµ‹è¯• n={n}...", end=" ", flush=True)

            # ç”Ÿæˆ CRS
            crs = keygen_crs(n, self.group)

            # ç”Ÿæˆéšæœºæ•°æ®å‘é‡ m (é•¿åº¦ N)
            m = [self.group.random(ZR) for _ in range(n)]
            gamma = self.group.random(ZR)

            # Header å¤§å°ï¼šç”Ÿæˆæ‰¿è¯º C
            C = commit_G(m, gamma, crs)
            C_bytes = objectToBytes(C, self.group)
            header_size = len(C_bytes)

            # Proof å¤§å°ï¼šç”Ÿæˆèšåˆè¯æ˜ Ï€_agg
            t = [self.group.random(ZR) for _ in range(n)]
            pi_agg = prove_agg_open(C, m, gamma, list(range(1, n+1)), t, crs)
            pi_bytes = objectToBytes(pi_agg, self.group)
            proof_size = len(pi_bytes)

            # Raw Data å¤§å°ï¼ˆåŸºå‡†å¯¹æ¯”ï¼‰ï¼šå‡è®¾æ¯ä¸ª ZR å…ƒç´ çº¦ 32 å­—èŠ‚
            raw_data_size = n * 32

            # è®°å½•ç»“æœ
            results['header_size'][n] = header_size
            results['proof_size'][n] = proof_size
            results['raw_data_size'][n] = raw_data_size

            print(f"âœ“ Header:{header_size}B Proof:{proof_size}B RawData:{raw_data_size}B")

        self.results['bandwidth'] = results
        return results
    
    def run_all_benchmarks(self, vector_sizes: List[int] = None, num_runs: int = 10):
        """
        è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•

        Args:
            vector_sizes: è¦æµ‹è¯•çš„å‘é‡å¤§å°åˆ—è¡¨
            num_runs: æ¯ä¸ªæµ‹è¯•é‡å¤çš„æ¬¡æ•°ï¼ˆé»˜è®¤10æ¬¡ï¼‰
        """
        if vector_sizes is None:
            vector_sizes = [4, 8, 16, 32, 64]

        print("\n" + "="*60)
        print("ğŸš€ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print(f"   æ¯ä¸ªæµ‹è¯•å°†é‡å¤ {num_runs} æ¬¡å¹¶è®¡ç®—å¹³å‡å€¼å’Œæ ‡å‡†å·®")
        print("="*60)

        self.benchmark_crs_generation(vector_sizes, num_runs)
        self.benchmark_commitments(vector_sizes, num_runs)
        self.benchmark_proofs(vector_sizes, num_runs)
        self.benchmark_verification(vector_sizes, num_runs)
        self.benchmark_memory(vector_sizes)
        self.benchmark_bandwidth(vector_sizes)

        print("\n" + "="*60)
        print("âœ… æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ")
        print("="*60)
    
    def save_results(self, filename='benchmark_results.json'):
        """ä¿å­˜ç»“æœåˆ° JSON æ–‡ä»¶"""
        import os
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(filename) if os.path.dirname(filename) else '.', exist_ok=True)

        data = {
            'timing': self.results,
            'memory': self.memory_results
        }
        with open(filename, 'w') as f:
            json.dump(data, f, indent=2)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ° {filename}")
    
    def print_summary(self):
        """æ‰“å°æ€§èƒ½æ€»ç»“"""
        print("\n" + "="*60)
        print("ğŸ“ˆ æ€§èƒ½æ€»ç»“")
        print("="*60)
        
        for category, data in self.results.items():
            print(f"\n{category.upper()}:")
            if isinstance(data, dict):
                for key, values in data.items():
                    if isinstance(values, dict):
                        print(f"  {key}:")
                        for n, t in values.items():
                            print(f"    n={n}: {t*1000:.2f} ms")
                    else:
                        print(f"  {key}: {values*1000:.2f} ms")


if __name__ == '__main__':
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    # num_runs=10 è¡¨ç¤ºæ¯ä¸ªæµ‹è¯•é‡å¤10æ¬¡ï¼Œè®¡ç®—å¹³å‡å€¼å’Œæ ‡å‡†å·®
    benchmark = PerformanceBenchmark('MNT224')
    benchmark.run_all_benchmarks([4, 8, 16, 32], num_runs=10)
    benchmark.save_results('try1028/benchmark_results.json')
    benchmark.print_summary()

